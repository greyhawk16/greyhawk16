{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc658906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: [WinError 127] 지정된 프로시저를 찾을 수 없습니다\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Q1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "training_epoch = 15\n",
    "batch_size = 100\n",
    "\n",
    "root = './data'  # data 라는 폴더를 생성\n",
    "\n",
    "# 훈련 시 사용할 데이터를 data 폴더에서 불러옴\n",
    "mnist_train = dset.MNIST(root = root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# 테스트를 진행할 때 사용할 데이터를 data 폴더에 서 불러옴\n",
    "mnist_test = dset.MNIST(root = root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "\n",
    "# 앞서 불러온 데이터들을 batch_size 단위로 나누기\n",
    "# 딥러닝 학습을 위해 필요함\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b9964c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3790, -0.5664,  1.1699,  ..., -0.4265,  0.9556, -1.2141],\n",
       "        [ 0.2521,  0.6304,  0.0862,  ...,  1.2181, -0.3647, -0.6537],\n",
       "        [-0.2817,  0.6753,  0.1631,  ...,  0.4837, -0.3218,  0.5194],\n",
       "        ...,\n",
       "        [ 0.5993, -2.0483,  0.9673,  ...,  0.9215, -0.3477,  0.1080],\n",
       "        [-1.7023, -0.4686, -1.4453,  ...,  1.2262,  1.6727, -1.3186],\n",
       "        [-0.3669,  0.2307, -0.2513,  ...,  0.1173, -0.5680,  0.8807]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "\n",
    "# 생성한 Tensor를 넘겨받을 장치를 선택\n",
    "# cuda가 사용 가능하면 cuda에, 아니라면 CPU\n",
    "# 같은 장소로 지정을 해야 Tensor 간 연산을 할 수 있음\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# 입력 데이터에 대해 선형변환을 해줌\n",
    "# 받는 데이터의 크기가 28 X 28 이므로, 28 ^2 = 784 를  input size에 넣어준다\n",
    "# MNIST는 0 ~ 9 까지의 가지므로 10을 OUTPUT_SIZE 로 설정\n",
    "# bias를 True로 해야 bias에 대해서도 학습이 가능\n",
    "linear = nn.Linear(784, 10, bias = True).to(device)\n",
    "\n",
    "\n",
    "# 입력되는 데이터는 그 값의 범위가 0 ~ 255 이고, Fully Connected Layer 단층의 모델이므로 역전파시 문제는 없음\n",
    "# linear.weight는 표준정규분포를 따르는 가중치로 초기화함.\n",
    "# batch normalization 방식을 사용하면, normal_ 에서 mean 과 std의 값을 세심히 설정 안해줘도 됨\n",
    "# tensorflow 는 xavier weight initialization 방식을 사용함\n",
    "nn.init.normal_(linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e33356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 \n",
    "\n",
    "# cross entropy loss를 계산할 준비를 함\n",
    "# tensor를 반환할 때 우리가 원하는 device로 반환\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "#  학습시 오류를 줄이기 위해 모델의 매개변수를 조정\n",
    "#  학습률은 0.1로, 확률적 경사하강법(SGD)을 적용\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60f5561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [1/15], Step [300/600], Loss: 1.6081, Accuracy: 66.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [1/15], Step [600/600], Loss: 1.2853, Accuracy: 77.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [2/15], Step [300/600], Loss: 0.7288, Accuracy: 85.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [2/15], Step [600/600], Loss: 0.5882, Accuracy: 86.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [3/15], Step [300/600], Loss: 1.4319, Accuracy: 85.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [3/15], Step [600/600], Loss: 0.8756, Accuracy: 82.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [4/15], Step [300/600], Loss: 0.6583, Accuracy: 83.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [4/15], Step [600/600], Loss: 0.8724, Accuracy: 82.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [5/15], Step [300/600], Loss: 0.6246, Accuracy: 88.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [5/15], Step [600/600], Loss: 0.6894, Accuracy: 90.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [6/15], Step [300/600], Loss: 0.4592, Accuracy: 87.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [6/15], Step [600/600], Loss: 0.7021, Accuracy: 84.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [7/15], Step [300/600], Loss: 0.6647, Accuracy: 87.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [7/15], Step [600/600], Loss: 0.9817, Accuracy: 87.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [8/15], Step [300/600], Loss: 0.6857, Accuracy: 82.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [8/15], Step [600/600], Loss: 0.3663, Accuracy: 86.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [9/15], Step [300/600], Loss: 0.4630, Accuracy: 94.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [9/15], Step [600/600], Loss: 0.7460, Accuracy: 83.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [10/15], Step [300/600], Loss: 0.4159, Accuracy: 89.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [10/15], Step [600/600], Loss: 0.3559, Accuracy: 88.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [11/15], Step [300/600], Loss: 0.4117, Accuracy: 87.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [11/15], Step [600/600], Loss: 0.4864, Accuracy: 87.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [12/15], Step [300/600], Loss: 0.6590, Accuracy: 87.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [12/15], Step [600/600], Loss: 0.3953, Accuracy: 91.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [13/15], Step [300/600], Loss: 0.2486, Accuracy: 94.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [13/15], Step [600/600], Loss: 0.5296, Accuracy: 87.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [14/15], Step [300/600], Loss: 0.4118, Accuracy: 89.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [14/15], Step [600/600], Loss: 0.2650, Accuracy: 93.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [15/15], Step [300/600], Loss: 0.5725, Accuracy: 86.00%\n",
      "after imgs shape:  torch.Size([100, 784])\n",
      "Epoch [15/15], Step [600/600], Loss: 0.3052, Accuracy: 91.00%\n"
     ]
    }
   ],
   "source": [
    "# Q4\n",
    "\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        \n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        \n",
    "        # torch.nn.Linear() 는 3차원 연산을 할 수 없기에, (데이터 개수) X (28 * 28) 의 형태로 img를 2차원의 형태로 펼침\n",
    "        \n",
    "        #변환 전 imgs 형태: [100, 1, 28, 28]\n",
    "        imgs = imgs.view(-1, 28 * 28)\n",
    "        # 변환 후: [100, 748]\n",
    "        \n",
    "        \n",
    "        outputs = linear(imgs)             # hypothesis(예측값)\n",
    "        loss = criterion(outputs, labels)  # 크로스 엔트로피를 계산하여 cost 값을 계산\n",
    "        \n",
    "        \n",
    "        # 모델 매개변수의 gradient를 명시적으로 0로 초기화\n",
    "        # gradient는 자동적으로 초기화되지 않으므로, 반드시 매 epoch 마다 0으로 설정 \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        #예측손실을 역전파\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        # step()를 이용, 역전파 단계에서 얻은 변화도로 매개변수를 조정\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # outputs의 행 별 최댓값을 argmax 에 저장\n",
    "        _, argmax = torch.max(outputs, 1)\n",
    "        \n",
    "        \n",
    "        # labels 의 argmax 에 대하여, 그 값들을 float로 바꾼 후, 그 평균을 구함\n",
    "        accuracy = (labels == argmax).float().mean()\n",
    "        \n",
    "        \n",
    "        if (i + 1) % 300 == 0:\n",
    "            print(\"after imgs shape: \", imgs.shape)\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, training_epoch, i + 1, len(train_loader), loss.item(), accuracy.item() * 100))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e587e640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of correct: 8873\n",
      "<class 'int'>\n",
      "Test Accuracy for 10000 images: 88.73%\n"
     ]
    }
   ],
   "source": [
    "# Q5\n",
    "\n",
    "# 루프 내에서 requires_grad 는 False로 설정, gradient 계산을 하지 않음\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "    for i, (imgs, labels) in enumerate(test_loader):\n",
    "        \n",
    "        imgs. labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        # torch.nn.Linear() 는 3차원 연산을 할 수 없기에, (데이터 개수) X (28 * 28) 의 형태로 img를 2차원의 형태로 펼침\n",
    "        imgs = imgs.view(-1, 28 * 28)\n",
    "        \n",
    "        outputs = linear(imgs)\n",
    "        \n",
    "#         print(torch.max(outputs, 1))\n",
    "        # _, 를 안해주면, 아래줄에서 bool 로 sum 연산을 하게 되므로 오류가 발생\n",
    "        _, argmax = torch.max(outputs, 1)\n",
    "        \n",
    "        # total은 총 데이터 개수\n",
    "        total += imgs.size(0)\n",
    "        \n",
    "        # correct는 labels 가 argmax와 같은 경우, 그 결과를 Ture(=1)로 저장 후 sum 연산을 수행함\n",
    "        # 예시) 10,000 개 중 8,900개가 True로 판정된 경우의 테스트 정확도 : 8,900 / 10,000 = 0.89 = 89 %\n",
    "#         print(labels == argmax)\n",
    "        correct += (labels == argmax).sum().item()\n",
    "        \n",
    "    \n",
    "    print(f\"No. of correct: {correct}\")\n",
    "    print(type(correct))\n",
    "    print('Test Accuracy for {} images: {:.2f}%'.format(total, correct / total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25142ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8597)\n",
      "Test Accuracy for 10000 images: 85.97%\n",
      "Label:  2\n",
      "Prediction:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# +@. 앞서 구축한 딥러닝 모델을 이용한 visualization\n",
    "\n",
    "\n",
    "\"\"\"----------------------------------Q5--------------------------------------\"\"\"\n",
    "# Q5의 코드 중 일부를 변경(선택사항입니다)\n",
    "\n",
    "\n",
    "# 루프 내에서 requires_grad 는 False로 설정, gradient 계산을 하지 않음\n",
    "with torch.no_grad():\n",
    "    correct = mnist_test.data.view(-1, 28 * 28).float().to(device)   #correct, total 을 tensor로 만들어야 오류가 안 생김\n",
    "    total = mnist_test.targets.to(device)\n",
    "    \n",
    "    \n",
    "    pred = linear(correct)  #correct는 tensor이어야 함\n",
    "    \n",
    "    \n",
    "    #  한 곳에서 torch.argmax(pred, 1)와 total의 값이 같은 경우(True), ans_pred의 해당 위치에 1을 넣음\n",
    "    ans_pred = torch.argmax(pred, 1) == total\n",
    "    \n",
    "    \n",
    "    # (ans_pred 에 저장된 원소들의 평균) = 테스트 정확도\n",
    "    # 예시) 10,000 개 중 8,900개가 True로 판정된 경우의 테스트 정확도 : 8,900 / 10,000 = 0.89 = 89 %\n",
    "    accuracy = ans_pred.float().mean()\n",
    "    print(accuracy)\n",
    "        \n",
    "    \n",
    "    print('Test Accuracy for {} images: {:.2f}%'.format(len(total), accuracy.item() * 100))\n",
    "    \n",
    "\n",
    "\"\"\"--------------------------------------------------------------------------\"\"\"    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "\n",
    "\n",
    "# 범위를 [r:r + 1]로 제한하여 이미지 개수 차원(1) 을 유지하며 그 차원이 [1, 28, 28]인 상태로 가져옴\n",
    "# torch.nn.Linear() 는 3차원 연산을 할 수 없기에, (데이터 개수) X (28 * 28) 의 형태로 img를 2차원의 형태로 펼침\n",
    "x_single_d = mnist_test.data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "y_single_d = mnist_test.targets[r : r + 1].to(device)\n",
    "single_pred = linear(x_single_d)\n",
    "\n",
    "\n",
    "print(\"Label: \", y_single_d.item())\n",
    "print(\"Prediction: \", torch.argmax(single_pred, 1).item())\n",
    "\n",
    "\n",
    "plt.imshow(mnist_test.data[r : r + 1].view(28, 28), cmap = \"Greys\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6190b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
